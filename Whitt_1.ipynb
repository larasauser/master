{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpmuNEKymLAlcwWTJeEtRn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larasauser/master/blob/main/Whitt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab notebook script : EGF (faithful) + Whittaker (kappa=1) pour Landsat + MODIS\n",
        "# - Charge Landsat clear (NDVI) et Landsat masked (NDVI with NaNs) depuis Google Drive\n",
        "# - Télécharge MODIS NDVI via Earth Engine, rééchantillonne sur grille Landsat et aligne temporellement\n",
        "# - Calcule M_reference (Eq.3-4), estime a,a0 (Eq.5), génère SLM, combine et lisse avec Whittaker (kappa=1)\n",
        "# - Exporte / sauvegarde les 6 reconstructions + calcule RMSE, R2, MAE, MS-SSIM, %reconstruction\n",
        "# Dépendances : earthengine-api, rasterio, numpy, scipy, scikit-image, tqdm, sklearn"
      ],
      "metadata": {
        "id": "fDJetAF7PiMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 0) Installer / importer libs et monter Drive / authentifier Earth Engine ===\n",
        "!pip install rasterio tqdm scikit-image sklearn\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling\n",
        "from rasterio.enums import Resampling as RIO_RES\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from scipy.linalg import lstsq\n",
        "from skimage.metrics import multiscale_structural_similarity as mssim\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "_wmzeNeAO2qa",
        "outputId": "0dce0459-9e17-4f52-9c67-ff219db2f58d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Using cached rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rasterio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1574438147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menums\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResampling\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mRIO_RES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rasterio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1) Paramètres principaux (à modifier si besoin) ===\n",
        "DRIVE_CLEAR_FOLDER = '/content/drive/MyDrive/landsat_clear'     # GeoTIFFs Landsat clear NDVI\n",
        "DRIVE_MASKED_FOLDER = '/content/drive/MyDrive/landsat_masked'   # GeoTIFFs Landsat masked NDVI (with NaN holes)\n",
        "OUTPUT_FOLDER = '/content/drive/MyDrive/egf_ws_outputs'         # outputs saved here\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# MODIS settings (change collection if you prefer another)\n",
        "MODIS_COLLECTION = 'MODIS/006/MOD13Q1'  # 16-day NDVI 250m (common choice); can be changed\n",
        "# Window used in paper: 200 m x 200 m\n",
        "WINDOW_METERS = 200.0\n",
        "PIXEL_SIZE_M = None  # will be inferred from first Landsat image (expected 30m)\n",
        "\n",
        "# Other parameters\n",
        "R_MIN_THRESHOLD = 0.0  # we'll use normalization per paper; keep threshold low but can be tuned\n",
        "MIN_OVERLAP = 3        # minimal time overlap to compute correlation/regression\n",
        "KAPPA = 1.0            # Whittaker smoothing parameter (paper uses kappa = 1)\n",
        "TILE_SIZE = 256        # if you want to tuiler (not used by default)\n",
        "# date parsing pattern (expects YYYYMMDD somewhere in filename)\n",
        "import re\n",
        "date_pattern = re.compile(r'(\\d{4})(\\d{2})(\\d{2})')\n",
        "\n",
        "def parse_date_from_filename(fn):\n",
        "    m = date_pattern.search(os.path.basename(fn))\n",
        "    if m:\n",
        "        return datetime.strptime(''.join(m.groups()), '%Y%m%d').date()\n",
        "    # fallback: file mtime\n",
        "    return datetime.fromtimestamp(os.path.getmtime(fn)).date()"
      ],
      "metadata": {
        "id": "FGk6QFzJO5u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2) Charger listes de fichiers Landsat clear & masked (assume they match in dates) ===\n",
        "clear_files = sorted(glob(os.path.join(DRIVE_CLEAR_FOLDER, '*.tif')))\n",
        "masked_files = sorted(glob(os.path.join(DRIVE_MASKED_FOLDER, '*.tif')))\n",
        "if len(clear_files)==0:\n",
        "    raise SystemExit(\"Aucun fichier trouvé dans DRIVE_CLEAR_FOLDER. Mets tes GeoTIFFs NDVI clear.\")\n",
        "if len(masked_files)==0:\n",
        "    raise SystemExit(\"Aucun fichier trouvé dans DRIVE_MASKED_FOLDER. Mets tes GeoTIFFs NDVI masked.\")\n",
        "\n",
        "# build date lists\n",
        "clear_dates = [parse_date_from_filename(f) for f in clear_files]\n",
        "masked_dates = [parse_date_from_filename(f) for f in masked_files]\n",
        "print(\"Found {} clear files and {} masked files\".format(len(clear_files), len(masked_files)))\n",
        "\n",
        "# We'll use the intersection of dates present in both collections to build the unified time axis.\n",
        "common_dates = sorted(list(set(clear_dates) | set(masked_dates)))\n",
        "print(\"Unified dates length (union):\", len(common_dates))\n"
      ],
      "metadata": {
        "id": "94OmTPOCO-dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3) Helper: read a single-band GeoTIFF into array + meta ===\n",
        "def read_singleband_tif(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        arr = src.read(1).astype(np.float32)\n",
        "        meta = src.meta.copy()\n",
        "    # convert nodata to np.nan if nodata set\n",
        "    nd = meta.get('nodata', None)\n",
        "    if nd is not None:\n",
        "        arr[arr==nd] = np.nan\n",
        "    return arr, meta\n",
        "\n",
        "# read first clear to infer projection/shape/transform\n",
        "arr0, meta0 = read_singleband_tif(clear_files[0])\n",
        "H, W = arr0.shape\n",
        "PIXEL_SIZE_M = abs(meta0['transform'][0])\n",
        "print(\"Inferred grid shape:\", H, W, \"pixel size (m):\", PIXEL_SIZE_M)"
      ],
      "metadata": {
        "id": "kT9mDvTCPBb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 4) Build time-indexed stacks (T,H,W) aligned on common_dates ===\n",
        "# Create dictionaries mapping date->file for clarity\n",
        "clear_map = {parse_date_from_filename(f): f for f in clear_files}\n",
        "masked_map = {parse_date_from_filename(f): f for f in masked_files}\n",
        "\n",
        "T = len(common_dates)\n",
        "landsat_clear_stack = np.full((T, H, W), np.nan, dtype=np.float32)\n",
        "landsat_masked_stack = np.full((T, H, W), np.nan, dtype=np.float32)\n",
        "dates = []\n",
        "\n",
        "for ti, d in enumerate(common_dates):\n",
        "    dates.append(d.isoformat())\n",
        "    if d in clear_map:\n",
        "        arr, _ = read_singleband_tif(clear_map[d])\n",
        "        landsat_clear_stack[ti] = arr\n",
        "    if d in masked_map:\n",
        "        arr, _ = read_singleband_tif(masked_map[d])\n",
        "        landsat_masked_stack[ti] = arr\n",
        "\n",
        "print(\"Stacks shapes:\", landsat_clear_stack.shape, landsat_masked_stack.shape)"
      ],
      "metadata": {
        "id": "GnYl3sX6PElp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5) Download MODIS NDVI time series via Earth Engine, resample to Landsat grid & align temporal axis ===\n",
        "# We'll request MODIS NDVI (collection) for the date range and for the Landsat footprint (meta0 transform)\n",
        "start_date = common_dates[0].isoformat()\n",
        "end_date = common_dates[-1].isoformat()\n",
        "print(\"MODIS date range:\", start_date, end_date)\n",
        "\n",
        "# compute bounds from meta0\n",
        "transform = meta0['transform']\n",
        "crs = meta0['crs']\n",
        "# compute bbox\n",
        "left = transform[2]\n",
        "top = transform[5]\n",
        "right = left + W * transform[0]\n",
        "bottom = top + H * transform[4]\n",
        "geom = ee.Geometry.Rectangle([left, bottom, right, top], proj=crs)\n",
        "\n",
        "# Helper to extract NDVI band from MODIS collection\n",
        "# MOD13Q1 has 'NDVI' band scaled by 0.0001 (check)\n",
        "modis_col = ee.ImageCollection(MODIS_COLLECTION).filterDate(start_date, end_date).filterBounds(geom)\n",
        "# Convert collection to list of images and their dates\n",
        "modis_list = modis_col.toList(modis_col.size())\n",
        "n_mod = modis_col.size().getInfo()\n",
        "print(\"Number of MODIS images in period (collection):\", n_mod)\n",
        "\n",
        "# We'll produce a MODIS stack aligned on the same dates (common_dates):\n",
        "# For each date in common_dates, we take the MODIS image with the exact same date if exists,\n",
        "# else we take the nearest MODIS image in time (within a window of +/- 16 days). This approximates the 8/16-day cadence.\n",
        "def ee_image_to_array(img, out_shape, out_transform, out_crs):\n",
        "    \"\"\"Fetch small image region from Earth Engine and return numpy array (single band).\"\"\"\n",
        "    # use getRegion to download as patches - more reliable for small areas\n",
        "    url = img.getDownloadURL({\n",
        "        'scale': int(PIXEL_SIZE_M),\n",
        "        'crs': out_crs,\n",
        "        'region': geom.toGeoJSONString(),\n",
        "        'format': 'GEO_TIFF'\n",
        "    })\n",
        "    # download via requests\n",
        "    import requests, tempfile, zipfile, io\n",
        "    r = requests.get(url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    # find the tif inside\n",
        "    tifname = [n for n in z.namelist() if n.endswith('.tif')][0]\n",
        "    z.extract(tifname, path='/content')\n",
        "    path = os.path.join('/content', tifname)\n",
        "    arr, meta = read_singleband_tif(path)\n",
        "    os.remove(path)\n",
        "    return arr\n",
        "\n",
        "# Build MODIS stack: for each common_date, find nearest MODIS image in collection\n",
        "modis_stack = np.full((T, H, W), np.nan, dtype=np.float32)\n",
        "\n",
        "# Build a map of MODIS image date -> ee.Image for quick nearest lookup\n",
        "modis_dates = []\n",
        "modis_images = []\n",
        "for i in range(n_mod):\n",
        "    img = ee.Image(modis_list.get(i))\n",
        "    ts = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd').getInfo()\n",
        "    modis_dates.append(datetime.strptime(ts, '%Y-%m-%d').date())\n",
        "    modis_images.append(img)\n",
        "\n",
        "# simple nearest search and download per date (may be slow for many dates; acceptable here)\n",
        "for ti, d in enumerate(common_dates):\n",
        "    # find nearest MODIS date\n",
        "    diffs = [abs((md - d).days) for md in modis_dates]\n",
        "    if len(diffs)==0:\n",
        "        continue\n",
        "    min_idx = int(np.argmin(diffs))\n",
        "    if diffs[min_idx] > 30:\n",
        "        # too far -> skip\n",
        "        continue\n",
        "    eeimg = modis_images[min_idx]\n",
        "    # select NDVI band (collection-specific): try 'NDVI' first, else 'NDVI'\n",
        "    try:\n",
        "        bandname = 'NDVI'\n",
        "        ee_band = eeimg.select(bandname)\n",
        "    except Exception:\n",
        "        raise SystemExit(\"Vérifie la collection MODIS choisie et le nom de bande NDVI.\")\n",
        "    # scale factor handling: many MODIS NDVI are scaled by 0.0001\n",
        "    # We'll download and then scale by 0.0001\n",
        "    print(f\"Downloading MODIS for date {d} (nearest {modis_dates[min_idx]}) ...\")\n",
        "    try:\n",
        "        arr = ee_image_to_array(ee_band, (H,W), transform, crs)\n",
        "        # scale\n",
        "        arr = arr.astype(np.float32) * 0.0001\n",
        "        arr[arr<-1] = np.nan\n",
        "        arr[arr>1] = np.nan\n",
        "        modis_stack[ti] = arr\n",
        "    except Exception as e:\n",
        "        print(\"MODIS download failed for date\", d, e)\n",
        "        continue\n",
        "\n",
        "print(\"MODIS stack built:\", modis_stack.shape)"
      ],
      "metadata": {
        "id": "zF0jEYJXPJIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6) EGF faithful implementation (Eq 3-5) helpers ===\n",
        "from math import floor\n",
        "\n",
        "def pearson_r(a, b):\n",
        "    \"\"\"Pearson r on 1D arrays with NaNs — returns np.nan if insufficient.\"\"\"\n",
        "    valid = ~np.isnan(a) & ~np.isnan(b)\n",
        "    if valid.sum() < 2:\n",
        "        return np.nan\n",
        "    x = a[valid].astype(float); y = b[valid].astype(float)\n",
        "    xm = x.mean(); ym = y.mean()\n",
        "    num = np.sum((x-xm)*(y-ym))\n",
        "    den = np.sqrt(np.sum((x-xm)**2)*np.sum((y-ym)**2))\n",
        "    if den == 0:\n",
        "        return np.nan\n",
        "    return num/den\n",
        "\n",
        "def compute_M_reference_pixel(i, j, modis_stack, sl_target_ts, radius_pixels):\n",
        "    \"\"\"\n",
        "    compute M_reference for pixel (i,j) following Eq(3)-(4).\n",
        "    - modis_stack: (T,H,W)\n",
        "    - sl_target_ts: 1D (T,) of SL target (landsat) at pixel (i,j)\n",
        "    returns M_ref (T,) or None if fallback\n",
        "    \"\"\"\n",
        "    T, H, W = modis_stack.shape\n",
        "    i0, i1 = max(0, i-radius_pixels), min(H, i+radius_pixels+1)\n",
        "    j0, j1 = max(0, j-radius_pixels), min(W, j+radius_pixels+1)\n",
        "    coords = [(ii,jj) for ii in range(i0,i1) for jj in range(j0,j1)]\n",
        "    corrs = []\n",
        "    mseries = []\n",
        "    for (ii,jj) in coords:\n",
        "        ms = modis_stack[:,ii,jj]\n",
        "        r = pearson_r(ms, sl_target_ts)\n",
        "        corrs.append(r)\n",
        "        mseries.append(ms)\n",
        "    corrs = np.array(corrs)\n",
        "    valid_idx = ~np.isnan(corrs)\n",
        "    if valid_idx.sum() == 0:\n",
        "        return None\n",
        "    cor_vals = corrs[valid_idx]\n",
        "    mseries_valid = [mseries[k] for k in np.where(valid_idx)[0]]\n",
        "    cor_min = cor_vals.min()\n",
        "    cor_max = cor_vals.max()\n",
        "    if cor_max - cor_min == 0:\n",
        "        R = np.ones_like(cor_vals)\n",
        "    else:\n",
        "        R = (cor_vals - cor_min) / (cor_max - cor_min)\n",
        "    if np.allclose(R, 0):\n",
        "        R = np.ones_like(R)\n",
        "    weights = R / R.sum()\n",
        "    Mref = np.zeros(T, dtype=float)\n",
        "    support = np.zeros(T, dtype=float)\n",
        "    for w, ms in zip(weights, mseries_valid):\n",
        "        valid_t = ~np.isnan(ms)\n",
        "        Mref[valid_t] += w * np.nan_to_num(ms[valid_t], nan=0.0)\n",
        "        support[valid_t] += w\n",
        "    Mref[support==0] = np.nan\n",
        "    return Mref\n",
        "\n",
        "def estimate_linear_transfer(M_ref, SL_ts):\n",
        "    \"\"\"Least squares estimate of a, a0 such that SL_ts ~ a*M_ref + a0\"\"\"\n",
        "    valid = ~np.isnan(M_ref) & ~np.isnan(SL_ts)\n",
        "    if valid.sum() < 2:\n",
        "        return None\n",
        "    A = np.vstack([M_ref[valid], np.ones(valid.sum())]).T\n",
        "    y = SL_ts[valid]\n",
        "    sol, *_ = lstsq(A, y, cond=None)\n",
        "    a, a0 = sol[0], sol[1]\n",
        "    return a, a0"
      ],
      "metadata": {
        "id": "IHClekn4PMAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7) Loop to compute a(x,y), a0(x,y), Mref_stack and SLM ===\n",
        "radius_pix = int(round((WINDOW_METERS/2.0) / PIXEL_SIZE_M))\n",
        "T, H, W = modis_stack.shape\n",
        "print(\"Window radius pixels:\", radius_pix)\n",
        "\n",
        "A = np.full((H,W), np.nan, dtype=float)\n",
        "A0 = np.full((H,W), np.nan, dtype=float)\n",
        "Mref_stack = np.full((T, H, W), np.nan, dtype=float)\n",
        "\n",
        "# We'll iterate only over pixels that are inside image (this loop can be slow; prints progress)\n",
        "print(\"Estimating M_reference and linear transfer a,a0 per pixel (this may take time)...\")\n",
        "for i in tqdm(range(H)):\n",
        "    for j in range(W):\n",
        "        sl_target_ts = landsat_clear_stack[:, i, j]  # SL target = landsat clear series for this pixel\n",
        "        Mref = compute_M_reference_pixel(i, j, modis_stack, sl_target_ts, radius_pix)\n",
        "        if Mref is None:\n",
        "            # fallback: local mean MODIS over window\n",
        "            i0, i1 = max(0, i-radius_pix), min(H, i+radius_pix+1)\n",
        "            j0, j1 = max(0, j-radius_pix), min(W, j+radius_pix+1)\n",
        "            block = modis_stack[:, i0:i1, j0:j1]\n",
        "            mref_alt = np.nanmean(block.reshape(T, -1), axis=1)\n",
        "            if np.all(np.isnan(mref_alt)):\n",
        "                continue\n",
        "            Mref = mref_alt\n",
        "        Mref_stack[:, i, j] = Mref\n",
        "        est = estimate_linear_transfer(Mref, sl_target_ts)\n",
        "        if est is None:\n",
        "            continue\n",
        "        a, a0 = est\n",
        "        A[i,j] = a\n",
        "        A0[i,j] = a0\n",
        "\n",
        "# Compute SLM = a*Mref + a0\n",
        "SLM = np.full((T,H,W), np.nan, dtype=float)\n",
        "for i in range(H):\n",
        "    for j in range(W):\n",
        "        if np.isnan(A[i,j]) or np.isnan(A0[i,j]):\n",
        "            continue\n",
        "        Mref = Mref_stack[:, i, j]\n",
        "        SLM[:, i, j] = Mref * A[i,j] + A0[i,j]"
      ],
      "metadata": {
        "id": "18Ud-vKOPQMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 8) Combine SLM and SL (use SL where present, else SLM) ===\n",
        "integrated = np.where(~np.isnan(landsat_clear_stack), landsat_clear_stack, SLM)"
      ],
      "metadata": {
        "id": "fJ7ZJCtTPVha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 9) Whittaker smoothing per-pixel (kappa = 1, second-difference penalty) ===\n",
        "def whittaker_smoother(y, kappa=1.0):\n",
        "    \"\"\"Whittaker smoothing on 1D array with NaNs (operate on contiguous valid segments).\"\"\"\n",
        "    n = y.shape[0]\n",
        "    mask = ~np.isnan(y)\n",
        "    if mask.sum() == 0:\n",
        "        return np.full_like(y, np.nan)\n",
        "    out = np.full_like(y, np.nan, dtype=float)\n",
        "    idx = np.where(mask)[0]\n",
        "    # find contiguous runs\n",
        "    runs = np.split(idx, np.where(np.diff(idx)!=1)[0]+1)\n",
        "    for run in runs:\n",
        "        s = run[0]; e = run[-1]+1\n",
        "        seg = y[s:e].astype(float)\n",
        "        m = len(seg)\n",
        "        if m <= 2:\n",
        "            out[s:e] = seg\n",
        "            continue\n",
        "        # second-difference operator D (m-2 x m)\n",
        "        D = np.zeros((m-2, m))\n",
        "        for r in range(m-2):\n",
        "            D[r, r] = 1\n",
        "            D[r, r+1] = -2\n",
        "            D[r, r+2] = 1\n",
        "        A = np.eye(m) + kappa * (D.T @ D)\n",
        "        z = np.linalg.solve(A, seg)\n",
        "        out[s:e] = z\n",
        "    return out\n",
        "\n",
        "print(\"Applying Whittaker smoothing per pixel (this may be slow)...\")\n",
        "smoothed = np.full_like(integrated, np.nan, dtype=float)\n",
        "for i in tqdm(range(H)):\n",
        "    for j in range(W):\n",
        "        smoothed[:, i, j] = whittaker_smoother(integrated[:, i, j], kappa=KAPPA)"
      ],
      "metadata": {
        "id": "_mn2KbzzPYns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 10) For the 6 target masked images, compute metrics vs truth (landsat_clear_stack) ===\n",
        "# Identify which indices correspond to the masked images that contain holes:\n",
        "# We'll assume user wants to evaluate on the dates where landsat_masked has NaNs (i.e., masked_files)\n",
        "target_indices = [ti for ti, d in enumerate(common_dates) if parse_date_from_filename(masked_map.get(d, ''))==d] if False else []\n",
        "# simpler: take dates present in masked_map (masked_files list) and map to common_indices\n",
        "target_indices = [ti for ti,d in enumerate(common_dates) if d in masked_map]  # indices where we had masked file\n",
        "print(\"Found target masked dates at indices:\", target_indices)\n",
        "if len(target_indices) == 0:\n",
        "    # fallback: choose 6 random dates where masked differs from clear\n",
        "    nontriv = [ti for ti in range(T) if np.any(np.isnan(landsat_masked_stack[ti]))]\n",
        "    target_indices = nontriv[:6]\n",
        "print(\"Evaluating on indices:\", target_indices)\n",
        "\n",
        "results = {}\n",
        "for t in target_indices:\n",
        "    truth = landsat_clear_stack[t]\n",
        "    recon = smoothed[t]\n",
        "    mask_was_missing = np.isnan(landsat_masked_stack[t])\n",
        "    # restrict to pixels that were masked and have truth (non-NaN)\n",
        "    eval_mask = mask_was_missing & ~np.isnan(truth)\n",
        "    if eval_mask.sum() == 0:\n",
        "        print(\"No eval pixels for date index\", t)\n",
        "        continue\n",
        "    y_true = truth[eval_mask].ravel()\n",
        "    y_pred = recon[eval_mask].ravel()\n",
        "    # compute metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    # MS-SSIM expects 2D images; we'll compute on full images (clamped to range -0.2..1 mapped to 0..1)\n",
        "    def scale01(x):\n",
        "        xmin, xmax = -0.2, 1.0\n",
        "        xx = np.clip(x, xmin, xmax)\n",
        "        return (xx - xmin) / (xmax - xmin)\n",
        "    try:\n",
        "        msssim = mssim(scale01(truth), scale01(recon), data_range=1.0)\n",
        "    except Exception:\n",
        "        msssim = np.nan\n",
        "    percent_recon = np.count_nonzero(~np.isnan(recon[mask_was_missing])) / max(1, np.count_nonzero(mask_was_missing)) * 100.0\n",
        "    results[dates[t]] = {'RMSE': float(rmse), 'R2': float(r2), 'MAE': float(mae), 'MS_SSIM': float(msssim), '%reconstruction': float(percent_recon)}\n",
        "    print(dates[t], results[dates[t]])"
      ],
      "metadata": {
        "id": "Jl0sdTR4PcBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 11) Save reconstructed images (the smoothed results) for the target dates to Drive ===\n",
        "def write_tif(path, arr, meta_template):\n",
        "    meta = meta_template.copy()\n",
        "    meta.update({'count': 1, 'dtype': 'float32', 'nodata': -9999})\n",
        "    out = np.nan_to_num(arr, nan=-9999).astype(np.float32)\n",
        "    with rasterio.open(path, 'w', **meta) as dst:\n",
        "        dst.write(out, 1)\n",
        "\n",
        "for t in target_indices:\n",
        "    outpath = os.path.join(OUTPUT_FOLDER, f'recon_{dates[t]}.tif')\n",
        "    write_tif(outpath, smoothed[t], meta0)\n",
        "    print(\"Saved\", outpath)\n",
        "\n",
        "# Save metrics CSV\n",
        "import csv\n",
        "csv_path = os.path.join(OUTPUT_FOLDER, 'metrics_results.csv')\n",
        "with open(csv_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['date','RMSE','R2','MAE','MS_SSIM','%reconstruction'])\n",
        "    for d, m in results.items():\n",
        "        writer.writerow([d, m['RMSE'], m['R2'], m['MAE'], m['MS_SSIM'], m['%reconstruction']])\n",
        "print(\"Metrics saved to\", csv_path)\n",
        "\n",
        "print(\"Finished. Résultats et reconstructions disponibles dans:\", OUTPUT_FOLDER)\n"
      ],
      "metadata": {
        "id": "V449NFt8Pf5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}