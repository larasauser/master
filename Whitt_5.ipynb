{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdQabdUobtEv067fkBnLIy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larasauser/master/blob/main/Whitt_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diNQyVY7rEkm"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio tqdm scikit-image scikit-learn --quiet\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling\n",
        "from scipy.linalg import lstsq\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dossiers ---\n",
        "DRIVE_CLEAR_FOLDER = '/content/drive/MyDrive/Whitt/NDVI_herens_Landsat8'\n",
        "DRIVE_MASKED_FOLDER = '/content/drive/MyDrive/Whitt/NDVI_herens_Landsat8_holes'\n",
        "DRIVE_MODIS_FOLDER = '/content/drive/MyDrive/Whitt/NDVI_herens_MODIS'\n",
        "OUTPUT_FOLDER = '/content/drive/MyDrive/Whitt/egfwh_herens_outputs_multisource2'\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Paramètres ---\n",
        "WINDOW_METERS = 200.0      # Taille de la fenêtre pour voisins MODIS\n",
        "KAPPA = 5.0                # Whittaker smoothing\n",
        "MIN_OVERLAP = 3            # Min nb de points pour calcul linéaire\n",
        "MODIS_WINDOW_DAYS = 16     # Nombre de jours autour de la date Landsat pour sélectionner MODIS\n"
      ],
      "metadata": {
        "id": "01PAJij3uAgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_date_from_filename(fn):\n",
        "    import re\n",
        "    m = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', os.path.basename(fn))\n",
        "    if m:\n",
        "        return datetime.strptime(''.join(m.groups()), '%Y%m%d').date()\n",
        "    return datetime.fromtimestamp(os.path.getmtime(fn)).date()\n",
        "\n",
        "def read_singleband_tif(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        arr = src.read(1).astype(np.float32)\n",
        "        meta = src.meta.copy()\n",
        "    nd = meta.get('nodata', None)\n",
        "    if nd is not None:\n",
        "        arr[arr==nd] = np.nan\n",
        "    return arr, meta\n",
        "\n",
        "def resample_to_target(src_arr, src_meta, target_meta):\n",
        "    dst_arr = np.full((target_meta['height'], target_meta['width']), np.nan, dtype=np.float32)\n",
        "    reproject(\n",
        "        source=src_arr,\n",
        "        destination=dst_arr,\n",
        "        src_transform=src_meta['transform'],\n",
        "        src_crs=src_meta['crs'],\n",
        "        dst_transform=target_meta['transform'],\n",
        "        dst_crs=target_meta['crs'],\n",
        "        resampling=Resampling.bilinear\n",
        "    )\n",
        "    return dst_arr"
      ],
      "metadata": {
        "id": "C9_F8yqy1Kbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Charger images Landsat ---\n",
        "clear_files = sorted(glob(os.path.join(DRIVE_CLEAR_FOLDER, '*.tif')))\n",
        "masked_files = sorted(glob(os.path.join(DRIVE_MASKED_FOLDER, '*.tif')))\n",
        "\n",
        "# --- Trouver la référence (plus petite image) ---\n",
        "sizes = []\n",
        "for f in clear_files:\n",
        "    arr, meta = read_singleband_tif(f)\n",
        "    sizes.append((arr.shape[0]*arr.shape[1], f, arr.shape, meta))\n",
        "sizes.sort()\n",
        "_, ref_file, ref_shape, ref_meta = sizes[0]\n",
        "H_ref, W_ref = ref_shape\n",
        "PIXEL_SIZE_M = abs(ref_meta['transform'][0])\n",
        "print(\"Reference Landsat:\", os.path.basename(ref_file), \"shape:\", ref_shape)\n",
        "\n",
        "# --- Stacker les images ---\n",
        "landsat_clear_stack, landsat_masked_stack = [], []\n",
        "clear_dates, masked_dates = [], []\n",
        "\n",
        "for f in clear_files:\n",
        "    arr, meta = read_singleband_tif(f)\n",
        "    if arr.shape != ref_shape:\n",
        "        arr = resample_to_target(arr, meta, ref_meta)\n",
        "    landsat_clear_stack.append(arr)\n",
        "    clear_dates.append(parse_date_from_filename(f))\n",
        "\n",
        "for f in masked_files:\n",
        "    arr, meta = read_singleband_tif(f)\n",
        "    if arr.shape != ref_shape:\n",
        "        arr = resample_to_target(arr, meta, ref_meta)\n",
        "    landsat_masked_stack.append(arr)\n",
        "    masked_dates.append(parse_date_from_filename(f))\n",
        "\n",
        "landsat_clear_stack = np.stack(landsat_clear_stack, axis=0)\n",
        "landsat_masked_stack = np.stack(landsat_masked_stack, axis=0)\n",
        "T, H, W = landsat_clear_stack.shape\n",
        "print(\"Stacks shapes (clear/masked):\", landsat_clear_stack.shape, landsat_masked_stack.shape)"
      ],
      "metadata": {
        "id": "aF9tic6L1PWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modis_files = sorted(glob(os.path.join(DRIVE_MODIS_FOLDER, '*.tif')))\n",
        "modis_map = {parse_date_from_filename(f): f for f in modis_files}\n",
        "modis_dates_list = sorted(modis_map.keys())\n",
        "\n",
        "# --- Créer un stack MODIS multiple par date Landsat ---\n",
        "modis_stack = np.full((T, H, W), np.nan, dtype=np.float32)\n",
        "\n",
        "for ti, d in enumerate(clear_dates):\n",
        "    # --- Sélectionner toutes les MODIS dans +/- MODIS_WINDOW_DAYS ---\n",
        "    selected_modis = [md for md in modis_dates_list if abs((md - d).days) <= MODIS_WINDOW_DAYS]\n",
        "    if len(selected_modis) == 0:\n",
        "        continue\n",
        "    mod_arrs = []\n",
        "    for md in selected_modis:\n",
        "        arr, meta = read_singleband_tif(modis_map[md])\n",
        "        arr_resampled = resample_to_target(arr, meta, ref_meta)\n",
        "        mod_arrs.append(arr_resampled)\n",
        "    # Moyenne de toutes les MODIS sélectionnées pour cette date Landsat\n",
        "    modis_stack[ti] = np.nanmean(np.stack(mod_arrs, axis=0), axis=0)\n",
        "\n",
        "print(\"MODIS stack aligned (multi-image):\", modis_stack.shape)\n"
      ],
      "metadata": {
        "id": "FYPUIwVk156z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5.1 Fonctions helpers EGF ===\n",
        "def pearson_r(a, b):\n",
        "    valid = ~np.isnan(a) & ~np.isnan(b)\n",
        "    if valid.sum() < 2:\n",
        "        return np.nan\n",
        "    x, y = a[valid], b[valid]\n",
        "    xm, ym = x.mean(), y.mean()\n",
        "    num = np.sum((x - xm) * (y - ym))\n",
        "    den = np.sqrt(np.sum((x - xm)**2) * np.sum((y - ym)**2))\n",
        "    if den == 0:\n",
        "        return np.nan\n",
        "    return num / den\n",
        "\n",
        "def compute_M_reference_pixel(i, j, modis_stack, sl_target_ts, radius_pix, corr_threshold=0.3):\n",
        "    T, H, W = modis_stack.shape\n",
        "    i0, i1 = max(0, i - radius_pix), min(H, i + radius_pix + 1)\n",
        "    j0, j1 = max(0, j - radius_pix), min(W, j + radius_pix + 1)\n",
        "    coords = [(ii, jj) for ii in range(i0, i1) for jj in range(j0, j1)]\n",
        "\n",
        "    corrs, mseries = [], []\n",
        "    for ii, jj in coords:\n",
        "        ms = modis_stack[:, ii, jj]\n",
        "        r = pearson_r(ms, sl_target_ts)\n",
        "        if np.isnan(r) or r < corr_threshold:\n",
        "            continue\n",
        "        corrs.append(r)\n",
        "        mseries.append(ms)\n",
        "\n",
        "    if len(corrs) == 0:\n",
        "        return None\n",
        "\n",
        "    R = np.array(corrs)\n",
        "    ms_valid = np.array(mseries)\n",
        "\n",
        "    # Normalisation des corrélations pour pondération\n",
        "    R = (R - R.min()) / (R.max() - R.min()) if R.max() != R.min() else np.ones_like(R)\n",
        "    weights = R / R.sum()\n",
        "\n",
        "    # Calcul de Mref pondéré\n",
        "    Mref = np.zeros(T, dtype=float)\n",
        "    support = np.zeros(T, dtype=float)\n",
        "    for w, ms in zip(weights, ms_valid):\n",
        "        valid_t = ~np.isnan(ms)\n",
        "        Mref[valid_t] += w * ms[valid_t]\n",
        "        support[valid_t] += w\n",
        "    Mref[support == 0] = np.nan\n",
        "    return Mref\n",
        "\n",
        "def estimate_linear_transfer(M_ref, SL_ts):\n",
        "    valid = ~np.isnan(M_ref) & ~np.isnan(SL_ts)\n",
        "    if valid.sum() < 2:\n",
        "        return None\n",
        "    A = np.vstack([M_ref[valid], np.ones(valid.sum())]).T\n",
        "    y = SL_ts[valid]\n",
        "    sol, *_ = lstsq(A, y)\n",
        "    return sol[0], sol[1]\n",
        "\n",
        "# === 5.2 Préparer les arrays pour reconstruction ===\n",
        "radius_pix = int(round((WINDOW_METERS / 2.0) / PIXEL_SIZE_M))\n",
        "A = np.full((H, W), np.nan)\n",
        "A0 = np.full((H, W), np.nan)\n",
        "Mref_stack = np.full((T, H, W), np.nan)\n",
        "\n",
        "# === 5.3 Calculer M_reference et coefficients linéaires pour tous les pixels ===\n",
        "print(\"Estimating M_reference and linear transfer for each pixel...\")\n",
        "for i in tqdm(range(H)):\n",
        "    for j in range(W):\n",
        "        sl_target_ts = landsat_clear_stack[:, i, j]\n",
        "        Mref = compute_M_reference_pixel(i, j, modis_stack, sl_target_ts, radius_pix)\n",
        "        if Mref is None:\n",
        "            # fallback: moyenne locale si pas de corrélation\n",
        "            i0, i1 = max(0, i - radius_pix), min(H, i + radius_pix + 1)\n",
        "            j0, j1 = max(0, j - radius_pix), min(W, j + radius_pix + 1)\n",
        "            block = modis_stack[:, i0:i1, j0:j1]\n",
        "            Mref = np.nanmean(block.reshape(T, -1), axis=1)\n",
        "        Mref_stack[:, i, j] = Mref\n",
        "        est = estimate_linear_transfer(Mref, sl_target_ts)\n",
        "        if est is not None:\n",
        "            A[i, j], A0[i, j] = est\n",
        "\n",
        "print(\"✅ M_reference and linear transfer computation done.\")\n"
      ],
      "metadata": {
        "id": "YtzQDTyM17Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 6.1 Whittaker smoother ===\n",
        "def whittaker_smoother(y, kappa=5.0):\n",
        "    n = y.shape[0]\n",
        "    mask = ~np.isnan(y)\n",
        "    if mask.sum() == 0:\n",
        "        return np.full_like(y, np.nan)\n",
        "\n",
        "    out = np.full_like(y, np.nan, dtype=float)\n",
        "    idx = np.where(mask)[0]\n",
        "    runs = np.split(idx, np.where(np.diff(idx) != 1)[0] + 1)\n",
        "\n",
        "    for run in runs:\n",
        "        s, e = run[0], run[-1] + 1\n",
        "        seg = y[s:e].astype(float)\n",
        "        m = len(seg)\n",
        "        if m <= 2:\n",
        "            out[s:e] = seg\n",
        "            continue\n",
        "        D = np.zeros((m - 2, m))\n",
        "        for r in range(m - 2):\n",
        "            D[r, r] = 1\n",
        "            D[r, r + 1] = -2\n",
        "            D[r, r + 2] = 1\n",
        "        A_mat = np.eye(m) + kappa * (D.T @ D)\n",
        "        out[s:e] = np.linalg.solve(A_mat, seg)\n",
        "    return out\n",
        "\n",
        "# === 6.2 Reconstruction temporaire avec coefficients linéaires ===\n",
        "SLM = np.full((T, H, W), np.nan)\n",
        "for i in range(H):\n",
        "    for j in range(W):\n",
        "        if not np.isnan(A[i, j]) and not np.isnan(A0[i, j]):\n",
        "            SLM[:, i, j] = Mref_stack[:, i, j] * A[i, j] + A0[i, j]\n",
        "\n",
        "# === 6.3 Injection des valeurs reconstruites uniquement sur les gaps + Whittaker ===\n",
        "final_stack = landsat_masked_stack.copy()  # start from masked images (NaN sur gaps)\n",
        "print(\"Merging reconstructed values into masked pixels and applying Whittaker smoothing...\")\n",
        "\n",
        "for t in range(len(masked_dates)):\n",
        "    gap_mask = np.isnan(final_stack[t])\n",
        "\n",
        "    smoothed_gap = np.full((H, W), np.nan)\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            if gap_mask[i, j]:\n",
        "                smoothed_gap[i, j] = whittaker_smoother(SLM[:, i, j], kappa=KAPPA)[t]\n",
        "\n",
        "    final_stack[t, gap_mask] = smoothed_gap[gap_mask]\n",
        "\n",
        "# === 6.4 Fonction pour sauvegarder GeoTIFFs ===\n",
        "def write_tif_nan(path, arr, meta_template):\n",
        "    meta = meta_template.copy()\n",
        "    meta.update({'count': 1, 'dtype': 'float32', 'nodata': None})\n",
        "    with rasterio.open(path, 'w', **meta) as dst:\n",
        "        dst.write(arr.astype(np.float32), 1)\n",
        "\n",
        "# === 6.5 Export final ===\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "print(\"Saving reconstructed GeoTIFFs...\")\n",
        "for t, date in enumerate(masked_dates):\n",
        "    outpath = os.path.join(OUTPUT_FOLDER, f'recon_{date}.tif')\n",
        "    write_tif_nan(outpath, final_stack[t], ref_meta)\n",
        "    print(\"Saved\", outpath)\n",
        "\n",
        "print(\"✅ Finished. Reconstructions available in:\", OUTPUT_FOLDER)\n"
      ],
      "metadata": {
        "id": "sK0VltkIb5Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "from rasterio.warp import reproject, Resampling\n",
        "import rasterio\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# === Dossiers ===\n",
        "hole_dir = '/content/drive/MyDrive/Whitt/NDVI_herens_Landsat8_holes'\n",
        "gt_dir = '/content/drive/MyDrive/Whitt/NDVI_herens_full'\n",
        "recon_dir = '/content/drive/MyDrive/Whitt/egfwh_herens_outputs_multisource2'\n",
        "\n",
        "# === Liste des fichiers ===\n",
        "hole_files = sorted(glob(os.path.join(hole_dir, '*.tif')))\n",
        "gt_files = sorted(glob(os.path.join(gt_dir, '*.tif')))\n",
        "recon_files = sorted(glob(os.path.join(recon_dir, 'recon_*.tif')))\n",
        "\n",
        "# === Fonctions utilitaires ===\n",
        "def read_tif_with_meta(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        arr = src.read(1).astype(np.float32)\n",
        "        meta = src.meta.copy()\n",
        "    arr[arr == meta.get('nodata', -9999)] = np.nan\n",
        "    return arr, meta\n",
        "\n",
        "def resample_to_match(source_arr, source_meta, target_meta):\n",
        "    dst_arr = np.empty((target_meta['height'], target_meta['width']), dtype=np.float32)\n",
        "    reproject(\n",
        "        source=source_arr,\n",
        "        destination=dst_arr,\n",
        "        src_transform=source_meta['transform'],\n",
        "        src_crs=source_meta['crs'],\n",
        "        dst_transform=target_meta['transform'],\n",
        "        dst_crs=target_meta['crs'],\n",
        "        resampling=Resampling.bilinear\n",
        "    )\n",
        "    return dst_arr\n",
        "\n",
        "# === Visualisation ===\n",
        "for i in range(len(hole_files)):\n",
        "    fname = os.path.basename(hole_files[i])\n",
        "    date = fname.replace('NDVI_', '').replace('.tif', '')\n",
        "\n",
        "    # Lecture de l'image trouée (référence)\n",
        "    hole, hole_meta = read_tif_with_meta(hole_files[i])\n",
        "\n",
        "    # Lecture et alignement des GT et reconstruction\n",
        "    gt, gt_meta = read_tif_with_meta(gt_files[i])\n",
        "    gt_resampled = resample_to_match(gt, gt_meta, hole_meta)\n",
        "\n",
        "    recon, recon_meta = read_tif_with_meta(recon_files[i])\n",
        "    recon_resampled = resample_to_match(recon, recon_meta, hole_meta)\n",
        "\n",
        "    # Crée un masque des gaps (NaN dans hole)\n",
        "    gap_mask = np.isnan(hole)\n",
        "\n",
        "    # Calcul de l'erreur uniquement sur les pixels des gaps\n",
        "    err = np.full_like(hole, np.nan)\n",
        "    valid_mask = gap_mask & ~np.isnan(gt_resampled) & ~np.isnan(recon_resampled)\n",
        "    err[valid_mask] = gt_resampled[valid_mask] - recon_resampled[valid_mask]\n",
        "    #err = gt_resampled - recon_resampled\n",
        "    # === Figure ===\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "    # === Création d'une colormap grise avec NaN en rouge ===\n",
        "    cmap_gray_red = plt.cm.gray\n",
        "    cmap_gray_red.set_bad(color='red')  # NaN en rouge\n",
        "\n",
        "    # 1) GAP\n",
        "    axs[0].imshow(np.ma.masked_invalid(hole), cmap=cmap_gray_red, vmin=-1, vmax=1)\n",
        "    axs[0].set_title(f'GAP - {date}', fontweight='bold')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # 2) GT\n",
        "    axs[1].imshow(np.ma.masked_invalid(gt_resampled), cmap=cmap_gray_red, vmin=-1, vmax=1)\n",
        "    axs[1].set_title(f'GT - {date}', fontweight='bold')\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    # 3) Reconstruction\n",
        "    axs[2].imshow(np.ma.masked_invalid(recon_resampled), cmap=cmap_gray_red, vmin=-1, vmax=1)\n",
        "    axs[2].set_title(f'RE - {date}', fontweight='bold')\n",
        "    axs[2].axis('off')\n",
        "\n",
        "    # 4) Erreur (GT - RE)\n",
        "    im4 = axs[3].imshow(np.ma.masked_invalid(err), cmap='RdYlGn')\n",
        "    axs[3].set_title('ERR (GT - RE)', fontweight='bold')\n",
        "    axs[3].axis('off')\n",
        "    plt.colorbar(im4, ax=axs[3], fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pLZJjfR_HiZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install piq"
      ],
      "metadata": {
        "id": "PCMSyAA9HzU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import torch\n",
        "import piq\n",
        "from glob import glob\n",
        "\n",
        "# === Dossiers ===\n",
        "hole_dir = '/content/drive/MyDrive/Whitt/NDVI_herens_Landsat8_holes'\n",
        "gt_dir = '/content/drive/MyDrive/Whitt/NDVI_herens_full'\n",
        "recon_dir = '/content/drive/MyDrive/Whitt/egfwh_herens_outputs_multisource2'\n",
        "\n",
        "# === Liste des fichiers ===\n",
        "hole_files = sorted(glob(os.path.join(hole_dir, '*.tif')))\n",
        "gt_files = sorted(glob(os.path.join(gt_dir, '*.tif')))\n",
        "recon_files = sorted(glob(os.path.join(recon_dir, 'recon_*.tif')))\n",
        "\n",
        "# === Fonctions utilitaires ===\n",
        "def read_tif_with_meta(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        arr = src.read(1).astype(np.float32)\n",
        "        meta = src.meta.copy()\n",
        "    arr[arr == meta.get('nodata', -9999)] = np.nan\n",
        "    return arr, meta\n",
        "\n",
        "def resample_to_match(source_arr, source_meta, target_meta):\n",
        "    \"\"\"Reprojette et recadre une image pour matcher une autre.\"\"\"\n",
        "    dst_arr = np.empty((target_meta['height'], target_meta['width']), dtype=np.float32)\n",
        "    reproject(\n",
        "        source=source_arr,\n",
        "        destination=dst_arr,\n",
        "        src_transform=source_meta['transform'],\n",
        "        src_crs=source_meta['crs'],\n",
        "        dst_transform=target_meta['transform'],\n",
        "        dst_crs=target_meta['crs'],\n",
        "        resampling=Resampling.bilinear\n",
        "    )\n",
        "    return dst_arr\n",
        "\n",
        "# === Fonction metrics ===\n",
        "def evaluate_metrics(gt_img, recon_img, gap_mask, image_name):\n",
        "    \"\"\"\n",
        "    Calcul des metrics uniquement sur les pixels manquants (gap_mask = True)\n",
        "    \"\"\"\n",
        "    valid_mask = gap_mask & ~np.isnan(gt_img) & ~np.isnan(recon_img)\n",
        "    if np.sum(valid_mask) == 0:\n",
        "        return pd.DataFrame([{\n",
        "            'Image name': image_name,\n",
        "            'RMSE': np.nan, 'R²': np.nan, 'MAE': np.nan,\n",
        "            'MS-SSIM': np.nan, '% reconstructed pixels': 0.0\n",
        "        }])\n",
        "\n",
        "    y_true = gt_img[valid_mask]\n",
        "    y_pred = recon_img[valid_mask]\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred) if np.var(y_true) > 1e-8 else np.nan\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # Normalisation pour MS-SSIM uniquement sur pixels valides\n",
        "    min_val = np.min([y_true.min(), y_pred.min()])\n",
        "    max_val = np.max([y_true.max(), y_pred.max()])\n",
        "    if max_val - min_val < 1e-8:\n",
        "        ms_ssim_score = np.nan\n",
        "    else:\n",
        "        gt_norm = (gt_img - min_val) / (max_val - min_val)\n",
        "        recon_norm = (recon_img - min_val) / (max_val - min_val)\n",
        "        gt_norm[~valid_mask] = 0.0\n",
        "        recon_norm[~valid_mask] = 0.0\n",
        "        gt_tensor = torch.from_numpy(gt_norm).unsqueeze(0).unsqueeze(0).float()\n",
        "        recon_tensor = torch.from_numpy(recon_norm).unsqueeze(0).unsqueeze(0).float()\n",
        "        ms_ssim_score = piq.multi_scale_ssim(gt_tensor, recon_tensor, data_range=1., reduction='none').item()\n",
        "\n",
        "    coverage = (np.sum(valid_mask) / np.sum(gap_mask)) * 100 if np.sum(gap_mask) > 0 else 0.0\n",
        "\n",
        "    results = {\n",
        "        'Image name': image_name,\n",
        "        'RMSE': rmse,\n",
        "        'R²': r2,\n",
        "        'MAE': mae,\n",
        "        'MS-SSIM': ms_ssim_score,\n",
        "        '% reconstructed pixels': coverage\n",
        "    }\n",
        "    return pd.DataFrame([results])\n",
        "\n",
        "# === Calcul des metrics pour toutes les images ===\n",
        "all_results = []\n",
        "\n",
        "for hole_path in hole_files:\n",
        "    fname = os.path.basename(hole_path)\n",
        "    date_str = fname.replace('NDVI_', '').replace('.tif', '')\n",
        "\n",
        "    # Lecture et alignement de l'image trouée\n",
        "    hole_img, hole_meta = read_tif_with_meta(hole_path)\n",
        "\n",
        "    # Ground truth\n",
        "    gt_path = [f for f in gt_files if date_str in f][0]\n",
        "    gt_img, gt_meta = read_tif_with_meta(gt_path)\n",
        "    gt_resampled = resample_to_match(gt_img, gt_meta, hole_meta)\n",
        "\n",
        "    # Reconstruction\n",
        "    recon_path = [f for f in recon_files if date_str in f][0]\n",
        "    recon_img, recon_meta = read_tif_with_meta(recon_path)\n",
        "    recon_resampled = resample_to_match(recon_img, recon_meta, hole_meta)\n",
        "\n",
        "    # Gap mask (pixels manquants dans l'image trouée)\n",
        "    gap_mask = np.isnan(hole_img)\n",
        "\n",
        "    # Metrics\n",
        "    df = evaluate_metrics(gt_resampled, recon_resampled, gap_mask, fname)\n",
        "    all_results.append(df)\n",
        "\n",
        "# Table finale\n",
        "metrics_table = pd.concat(all_results, ignore_index=True)\n",
        "metrics_table"
      ],
      "metadata": {
        "id": "yO0LW5HaH4y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Paramètres du pixel / patch ---\n",
        "i_pixel, j_pixel = 100, 120  # exemple, à adapter\n",
        "patch_size = 1  # si patch_size >1, extraire la moyenne du patch\n",
        "\n",
        "# --- Fonctions helper ---\n",
        "def extract_patch_stack(stack, i, j, size=1):\n",
        "    \"\"\"Retourne la série temporelle pour un pixel ou un patch (moyenne).\"\"\"\n",
        "    T, H, W = stack.shape\n",
        "    half = size // 2\n",
        "    i0, i1 = max(0, i-half), min(H, i+half+1)\n",
        "    j0, j1 = max(0, j-half), min(W, j+half+1)\n",
        "    patch = stack[:, i0:i1, j0:j1]\n",
        "    return np.nanmean(patch.reshape(T, -1), axis=1)\n",
        "\n",
        "# --- Extraire les séries ---\n",
        "ndvi_clear_full = extract_patch_stack(landsat_clear_stack, i_pixel, j_pixel, patch_size)\n",
        "ndvi_recon_full = extract_patch_stack(final_stack, i_pixel, j_pixel, patch_size)\n",
        "ndvi_whittaker_full = extract_patch_stack(SLM, i_pixel, j_pixel, patch_size)\n",
        "\n",
        "# Création de la série masked alignée sur la stack complète\n",
        "ndvi_masked_full = np.full_like(ndvi_clear_full, np.nan, dtype=float)\n",
        "for t, date_masked in enumerate(masked_dates):\n",
        "    # trouver l'indice de la date la plus proche dans clear_dates\n",
        "    diffs = [abs((cd - date_masked).days) for cd in clear_dates]\n",
        "    idx = np.argmin(diffs)\n",
        "    ndvi_masked_full[idx] = extract_patch_stack(landsat_masked_stack, i_pixel, j_pixel, patch_size)[t]\n",
        "\n",
        "# --- Dates pour l'axe X ---\n",
        "dates_dt = clear_dates  # on prend la stack complète Landsat\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.plot(dates_dt, ndvi_clear_full, 'o-', color='blue', label='Landsat original')\n",
        "plt.plot(dates_dt, ndvi_masked_full, 'o-', color='orange', label='Landsat masked')\n",
        "plt.plot(dates_dt, ndvi_whittaker_full, 'o-', color='green', label='Whittaker intermediate')\n",
        "plt.plot(dates_dt, ndvi_recon_full, 'o-', color='red', label='Reconstruction finale')\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"NDVI\")\n",
        "plt.title(f\"Série temporelle NDVI pixel ({i_pixel}, {j_pixel})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xkisbmwukl8V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}